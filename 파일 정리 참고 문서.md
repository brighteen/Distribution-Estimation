## 정리어떻게 할지
- 처음 GMM부터 현재 선형보간, 논문 리뷰한것들 체계적으로 정리
- 어느정도 정리가 될쯤 폴더 그대로 레포지토리 만들고 대학원 컨텍때 사용할것.
- 서버 파일들 참고.
- 각 파일에서 무슨 실험을 했는지, 이전 실험과 어떤 차이가 있는지, 결과를 얼마나 뜯어봤는지.
- 스크린샷에 대한 출처 명확히 기재.
- 날짜로 구분하는게 좋을지...
- 표기(notation)을 명확히 할것.
- 탭에 필기한것도 보고 같이 첨부
- 파일 크기 좀 줄이자(출력 로그 삭제하고 결과는 따로 저장)
- 논문은 어떤 논문을 참고했는지 기재만 하기(ex. Self-consuming generative models go MAD, figure1. ~~~)

## 이전에 한것들 간단하게 정리(좀 부풀리거나 실험한 코드들 보완해서 업로드할 것.)

### 0827
1. GMM의 컴포넌트를 최적화
2. 계층적 GMM(BIC, AIC, 교차검증 등 지표 사용) -> 잠재 공간의 데이터들을 여러 분포로 잘 나누지 못함.
3. 잠재 공간 자체가 타당한가? 증명. VAE의 KL손실이 강하게 적용되어 GMM이 봤을때 잠재공간 내 데이터가 하나의 단일 정규분포를 따르게끔 학습되었기어 여러개의 가우시안으로 나누지 못한다고 생각이 듦.
4. 만약 모델이 학습한 잠재공간이 이상적인 학습을 했다면 잠재 공간은 연속적, convex한 공간이어야 함.(그래도 256차원이라 데이터가 희소할거임, 모든 공간이 연속적이라고 기대하기는 힘듦.)
5. 그럼 애초에 잠재공간에 non-convex한 공간을 만들지 말자고 주장하는 벡터 양자화를 만남.
    - 연속적인 잠재공간 대신 이산적인 코드북만 사용하자. 그럼 보간은?
6. 잠재 공간 내 데이터끼리 유클리드 거리를 재봄. 생각보다 가까운거같은데?

### 생각.
- 기존 학습 데이터 포인트들은 고차원 공간 내 convex hull 내부 혹은 경계에 위치(모델이 암기한 부분)
- 새로운 test 데이터는 모두 경계 밖에 위치 = 외삽 영역(모델의 응용력을 검증? 판별?)
- 정말 좋은 학습을 통해 만든 모델의 성능은 convex hull 내부가 아닌 외삽 영역을 효과적으로 일반화 해야한다고 생각.
- Model의 크기가 커져 학습데이터를 완벽하게 보간(train loss=0)하는 지점을 지나면 오히려 test성능이 다시 향상되는 이중 강하(double descent)일때 모델이 학습데이터들을 활용해 외삽으로 나갈 수 있을까 생각.

### 실험
- GMM으로 k=500 학습
    1. 각 컴포넌트에서 샘플링해봐
        - 샘플링한게 괜찮다?(얼굴의 형태를 유지한다) 보간 영역 방향으로 추가 샘플링(근데 보간 영역은 두 데이터 사인데 그 너머로 어떻게 가지..)
        - 많이 맞은 얼굴이 복원된다? 보간한 영역이 외삽 영역으로 빠졌다고 생각이 듦.
    2. 각 컴포넌트의 평균과 분산 봐봐
    3. 잠재공간에 데이터들을 2,3차원으로 보내봐.
    
### 생각.
- 최종 goal은 모든 컴포넌트에서 샘플링할 시에 외삽영역을 제외해야 함...(근데 외삽 영역이 비로소 우리가 가야하는 곳이 아닌지)
- 그럼 모델의 일반화 능력은 convex hull 내부에서만 일어나는건지?
- 고차원에서의 학습은 주로 외삽이 아닌지?
- 고차원에서 샘플링했을때 내삽 영역에서 뽑을 확률이 너무 낮은게 아닌지..
- 차라리 잠재 공간의 차원$\mathbb{R}^{256}$를 더 줄이면 안되나?
- 임의의 데이터포인트를 기준으로 256개의 축중 하나의 축 방향으로 일정 상수 k만큼 이동한다면 어떻게 될까?(그렇게 256축으로 모두 이동하다보면 뭐라도 보이지 않을까?)


### 실험
- 분산이 작은 컴포넌트는 학습 데이터와 거의 유사한 데이터밖에 생성하지 못함. + 노이즈가 거의 없음.
- 분산이 큰 컴포넌트는 비교적 다양하지만 그만큼 노이즈도 껴있음.

- FID 실험(이건 안해봄 계획만)
    - 클러스터 k를 여러개 지정해서 gmm 따로 학습
    - 각 gmm으로 샘플링 5만장정도.
    - 실제 이미지와 각 gmm이 생성한 이미지 사이 FID 점수 비교 pytorch-fid 라이브러리.

## KDE 부분
- KDE의 하이퍼파라미터 h(bandwidth)를 알맞게 조절해야 함.
- 보통 h가 작으면 데이터와 유사한 데이터를 샘플링.

- 모든 데이터의 평균과 가까운 저차원 잠재벡터와(데이터들의 중심에 있는 포인트) / 제일 먼 데이터의 h차이 비교. (데이터와 거리 차이와 h의 관계 파악)
- 20만 차원에서 256으로 내려왔지만 256차원도 결국 고차원임. -> 평균벡터가 무의미?
- h에 따라 모든 축방향으로 샘플링하다보면 데이터포인트를 중심으로 별사탕같은 형태를 생각해볼 수 있지 않을까?

### 생각.
- 두 데이터포인트 사이를 보간했을떄 나오는 유의미한 결과들 방향의 분포에 가중치를 더 주고 나머지 안좋은 결과 방향은 가중치를 내려서 샘플링.
- 현재 2만개 데이터끼리의 각 선형 보간 영역을 하나의 균등분포라고 한다면 이 2만개 데이터의 모집단을 보다 더 근사할 수 있을거같음.
애초에 미관심영역이 포함된 분포(가우시안 분포)로 보지말고 두 데이터쌍의 선형보간 경로를 path로 잡은 다음 그 path들을 분포로 보고 샘플링하자. 왜? 대부분의 path에서 보간이 자연스럽게 이어지니까.
이때 정규분포에서 샘플링하는 것보다 이 균등분포에서 하는게 더 모집단에 근접한 데이터들을 얻을 수 있지 않냐 생각. 합당하지?

0829
- 선형보간 시 보간인자 알파가 0, 1일때 복원된 벡터와 원본은 다른 벡터임. 왜냐하면 인코더와 디코더를 거치면서 불가피한 정보손실이 일어났기 때문에.
- 두 이미지 사이 거리를 잠재공간, 픽셀공간에서 모두 계산..(거리를 잰다고 해도 우리 관심 대상은 비유클리드한 매니폴드라 의미가 있을까 생각이 듦.)

### 중간 개념 정리.
- 저차원 잠재 공간은 고차원 내 정답 데이터들이 존재하는 저차원 True 매니폴드를 학습한 유클리드 공간.
- Q. 어떤 잠재벡터가 잠재 공간 내 매니폴드에 속해있지 않지만, 참 매니폴드에 속할 수 있을까?(이것이 모델이 완벽한 일반화에 성공한 이상적인 경우라는데...) 즉, 학습된 매니폴드를 벗어난 입력이 들어와도, 그 결과물이 참 매니폴드 위에 존재할 수 있는 이론적 가능성임.
- 하지만 실제로는 학습 데이터에 크게 의존하기에 학습된 매니폴드를 벗어나는 입력이 들어오면 참 매니폴드를 벗어나는 결과를 만들어낼 가능성이 매우 높음.


### 생각.
- 보간했을 때 좋은 결과들을 가지고 다시 학습시키면 모델이 알고있는 매니폴드 영역이 참 매니폴드에 가까워질까?
- 가장 좋은 보간은 학습된 매니폴드의 측지거리대로 데이터를 움직이는 건데... 현재는 선형보간밖에 못하는거고, 근데 생각보다 선형보간 결과가 유의미함.(두 데이터는 선형적인 관계를 띄는건지?)
- 선형 보간 결과끼리의 선형보간을 하면? 이것을 반복하면 참 매니폴드에 가까워지지 않을까? 단, 보간 결과가 타당한지 정량적으로 평가해야 함.
- 반복적 LLE로 저차원 참 매니폴드의 측지거리를 조금씩 찾아가다보면 전체 매니폴드를 찾을 수 있지 않을까?
    - 국소의 기준을 어떻게 잡을지, 256개의 방향중 어디로 갈지, 어디까지가 매니폴드인지 정의할 수 있는가?
- 선형 보간보다는 비선형보간이 더 유리할텐데, 그 비선형구조를 어떻게 찾지? -> 신경망으로 찾을 수 있을까?

# **"모집단의 분포를 섬세하게 추정하는 것"**

### 생각.
- 선형 보간 방정식은 두 벡터의 선형 결합임. a*x+(1-a)*y = c_1*x_1+c_2*x_2.
- 3점 보간은 1-a-b=0을 만족하는 상수 c를 적용하면 됨.
- 이 선형결합말고 다른 선형대수의 method를 이용하면 어떨까?
- 이 저차원 잠재공간으 유클리드한 공간이니까 사용가능함.
- 한 벡터에 대해 나머지 19,999개의 잠재벡터와 보간해볼까?
- 256차원을 span하는 벡터를 찾는다면? 모든 축이 직교하고, 256개의 벡터가 필요함.

- 특징 벡터에 대해서도 생각.
- 웃는 남자 - 남자 + 여자 = 웃는 여자. 이때 '웃음'이라는 벡터를 찾을 수 있을까?
- 특징 벡터에 스칼라곱을 하면?
- styleGAN참고.
- 다양체 가설을 따르는 여러 선형, 비선형차원축소방법들 보기.(LLE, Isomap, t-SNE 등)

### 레전드 미친 보간 네트워크 설계
학습된 매니폴드의 휘어짐(비선형성)에 대한 함수를 신경망으로 찾을 수 있을까?
[보간네트워크](<0830 보간 넷 1.md>)

이 보간네트워크는 두 점 사이의 최적의 곡선 경로를 찾는 것.
학습된 매니폴드를 더 좋게 만드는 것 아님.

#### 결과
- 학습 안됨.
- 서브실험에서 재투영할 수록 얼굴의 특징이 과장되어 복원된다는 것을 확인함. -> 잘 학습한 모델이라면 재투영할 수록 학습된 매니폴드에 가까워질 것임이라는 초기 가설을 틀렸다고 말하고 있음.
- 하나의 데이터쌍에 대해 알파(a)값을 충분히 많이 줘야 함. 그래야 두 데이터 사이를 어느정도 매꿀 수 있음.
- 재투영오차+판별자를 동시에 최소화(안되면 따로 최소화하는것도 생각.) -> 이것도 안됨.
- 서브실험으로 3차원에 간단한 구를 그리고 보간네트워크를 학습시켜 구의 표면을 따라가는지 확인. -> 얘는 잘 찾는거같은데?

- 0907_다시 생각해보니 손실함수에서 선형보간한 벡터와 재투영한 벡터의 차이를 최소화해야하는데 그럼 움직이는 애가 재투영한 벡터라 계속 선형보간한 벡터 근처에 머물을거임. 이게 합당하려면 잠재공간 내 정답 벡터가 존재해야함. 이때 정답벡터라 하면 두 데이터 사이에 실제 데이터가 존재해야 한다는 것임. 마치 연속적인 데이터를 말하는 것과 비슷한 것.


#### 보간네트워크에 대해 추가로 할 것들
- 특정 데이터들끼리만 학습한 후에 그 근처에서 학습한 경로에서 샘플링해보기.
- geodesic learning 논문에서 제시한 손실함수 도입.


### 관련 논문
#### 1. 매니폴드 학습 기반 실내 위치 보간
- 매니폴드의 국소적 선형성을 LLE 기반 보간

#### 2. 데이터 매니폴드에서 균일한 보간을 통한 측지학습
- 오토인코더로 잠재공간을 평평하게 핌.
- 이 평평한 공간에서 기하학적 손실로 최적의 경로 학습

#### 3. 생성모델에서 잠재공간 보간을 통한 비지도 메타학습
- 데이터증강의 상위버전 -> 잠재공간 보간이라고 명명.

#### 4. 잠재공간 형성을 통한 오토인코더 이미지 보간
- 오토인코더의 보간 결과물에 생기는 이상한 현상(artifact)은 잠재공간 매니폴드가 부드럽지 않고 볼록?하기 때문.(non convex가 맞지않나?)

#### 5. 딥 생성모델의 잠재공간에서의 선형보간에 대해
- MNIST 데이터를 학습한 2차원 VAE 잠재공간의 기하학적 속성.

#### 6 정량적 평가 프레임워크를 통한 잠재공간 보간 재검토



---


### 0910
- 고차원에서 데이터를 다루는 건 너무 위험함.
- 복잡한 사고를 할 필요가 없음.
- L2norm과 제곱오차의 차이가 뭘까?
- n-점 보간은 각 데이터 앞 스칼라값들이 (1-a-b-c...-n)=0만 만족하는 convex set 내에서 찾으면 됨.
