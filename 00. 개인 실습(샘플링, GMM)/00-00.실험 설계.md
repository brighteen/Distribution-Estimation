## 가우시안 믹스쳐 모델(GMM) 평가를 위한 실험 설계 및 Python 코드

### 1\. 실험 설계 제안 🧪

GMM은 전체 데이터가 여러 개의 가우시안(정규) 분포가 혼합되어 생성되었다고 가정하는 모델입니다. 따라서 저희의 실험 목표는 **이차함수라는 비선형적인 데이터 분포를 몇 개의 가우시안 분포로 가장 잘 근사(approximate)할 수 있는지**를 찾는 것입니다.

실험은 다음과 같은 단계로 설계합니다.

1.  **데이터 생성 (Data Generation):**

      * 간단한 이차함수($y = ax^2 + bx + c$)를 정의합니다.
      * 이 함수를 따라 분포하는 1000개의 데이터 포인트를 생성합니다.
      * 실제 데이터처럼 보이도록 약간의 무작위 노이즈(noise)를 추가합니다.

2.  **모델링 가정 (Hypothesis):**

      * 생성된 이차함수 데이터는 단일 정규분포를 따르지 않습니다.
      * 우리는 "이 데이터를 **여러 개(k개)의 가우시안 분포의 혼합으로 표현할 수 있다**"고 가정합니다.
      * 실험의 핵심은 최적의 `k`값을 찾는 것입니다.

3.  **모델 학습 및 평가 (Modeling & Evaluation):**

      * GMM 모델의 핵심 파라미터인 컴포넌트(가우시안 분포) 개수 `n_components`를 1부터 N까지 변화시키며 여러 GMM 모델을 학습시킵니다.
      * 각 모델에 대해 \*\*AIC(Akaike Information Criterion)\*\*와 **BIC(Bayesian Information Criterion)** 정보량 기준 값을 계산하여 평가합니다.
          * **AIC/BIC**: 모델이 데이터에 얼마나 잘 맞는지를 나타내는 '적합도'와 모델의 복잡도(사용된 파라미터 수) 사이에 균형을 잡아주는 지표입니다. **이 값이 낮을수록 더 좋은 모델**로 평가됩니다. 일반적으로 BIC가 더 선호됩니다.
      * 가장 낮은 BIC 값을 보인 모델을 최적 모델로 선정합니다.

4.  **결과 시각화 및 해석 (Visualization & Interpretation):**

      * 원본 데이터 분포를 시각화합니다.
      * 컴포넌트 개수에 따른 AIC/BIC 값의 변화를 그래프로 그려 최적의 지점을 확인합니다.
      * 최종 선택된 GMM이 데이터를 어떻게 클러스터링했는지, 그리고 각 가우시안 분포가 어떻게 데이터를 설명하는지 시각화하여 확인합니다.

-----

### 2\. 핵심 질문에 대한 답변

> 그 데이터들이 어떤 정규분포를 따르는지 유추부터 해보면 좋을까?

**아니요, 그럴 필요가 없으며 GMM의 목적과 다릅니다.**

이차함수에서 샘플링된 데이터는 이름 그대로 '이차 함수' 형태의 분포를 가지며, 종 모양의 단일 정규분포와는 거리가 멉니다. 만약 이 데이터를 단일 정규분포로 근사하려 하면, 아래 그림의 왼쪽처럼 데이터의 특징을 전혀 잡아내지 못하는 결과가 나옵니다.

**GMM을 사용하는 이유가 바로 여기에 있습니다.** GMM은 이처럼 복잡하고 비선형적인 데이터 분포를 **여러 개의 간단한 정규분포의 조합으로 유연하게 근사**하기 위해 만들어진 모델입니다. 따라서 '어떤 단일 정규분포를 따를까?'를 고민하는 대신, \*\*'몇 개의 정규분포를 섞으면 이 데이터를 가장 잘 설명할 수 있을까?'\*\*를 질문하는 것이 올바른 접근 방식입니다.

-----

### 3\. Python 전체 코드 예제 💻

아래 코드는 위에서 설계한 실험 전체를 구현한 것입니다.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture
from matplotlib.patches import Ellipse
import warnings

# 경고 메시지 무시
warnings.filterwarnings("ignore")

# 1. 데이터 생성 (Data Generation)
# y = 0.1x^2 - 0.8x + 5 형태의 이차함수 + 노이즈
np.random.seed(42)
n_samples = 1000
X_func = np.linspace(-10, 15, n_samples)
Y_func = 0.1 * X_func**2 - 0.8 * X_func + 5 + np.random.randn(n_samples) * 1.5

# GMM은 (n_samples, n_features) 형태의 입력을 기대하므로 데이터를 재구성
data = np.vstack([X_func, Y_func]).T

print(f"데이터 형태: {data.shape}")


# 2. GMM 모델 학습 및 평가 (Modeling & Evaluation)
# n_components를 1부터 10까지 변화시키며 AIC/BIC 비교
n_components_range = range(1, 11)
bic_scores = []
aic_scores = []
gmm_models = []

for n_components in n_components_range:
    # GMM 모델 생성 및 학습
    gmm = GaussianMixture(n_components=n_components, random_state=42)
    gmm.fit(data)
    
    # 모델 및 점수 저장
    gmm_models.append(gmm)
    bic_scores.append(gmm.bic(data))
    aic_scores.append(gmm.aic(data))

# 가장 낮은 BIC를 가진 모델을 최적 모델로 선택
best_gmm_index = np.argmin(bic_scores)
best_gmm = gmm_models[best_gmm_index]
best_n_components = best_gmm.n_components

print(f"최적의 컴포넌트 개수 (BIC 기준): {best_n_components}")


# 3. 결과 시각화 (Visualization)

plt.style.use('seaborn-v0_8-whitegrid')
fig, axes = plt.subplots(1, 3, figsize=(24, 7))
plt.rc('font', family='Malgun Gothic') # 한글 폰트 설정

# 3-1. 원본 데이터 시각화
axes[0].scatter(data[:, 0], data[:, 1], s=10, alpha=0.7)
axes[0].set_title('1. 원본 데이터 (이차함수 샘플링)', fontsize=15)
axes[0].set_xlabel('X')
axes[0].set_ylabel('Y')

# 3-2. AIC/BIC 점수 시각화
axes[1].plot(n_components_range, bic_scores, 'bo-', label='BIC')
axes[1].plot(n_components_range, aic_scores, 'ro-', label='AIC')
axes[1].axvline(best_n_components, color='green', linestyle='--', 
                label=f'Best n_components = {best_n_components}')
axes[1].set_title('2. 컴포넌트 개수에 따른 BIC & AIC', fontsize=15)
axes[1].set_xlabel('컴포넌트 개수 (Number of components)')
axes[1].set_ylabel('정보량 기준 (Information Criterion)')
axes[1].legend()

# 3-3. 최적 GMM 결과 시각화
splot = axes[2]
Y_ = best_gmm.predict(data)

# 각 클러스터에 대해 다른 색상으로 데이터 표시
for i in range(best_n_components):
    splot.scatter(data[Y_ == i, 0], data[Y_ == i, 1], s=10,
                   label=f'Cluster {i+1}')

# GMM이 학습한 가우시안 분포를 타원으로 그리는 함수
def draw_ellipse(position, covariance, ax=None, **kwargs):
    ax = ax or plt.gca()
    
    # 공분산 행렬에서 고유값 분해로 타원의 주축 찾기
    if covariance.shape == (2, 2):
        U, s, Vt = np.linalg.svd(covariance)
        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))
        width, height = 2 * np.sqrt(s)
    else: # 1D 공분산의 경우 (대각 행렬)
        angle = 0
        width, height = 2 * np.sqrt(covariance)
    
    # 95% 신뢰구간에 해당하는 타원 그리기 (카이제곱 분포)
    for nsig in range(1, 4):
        ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle, **kwargs))

# 각 가우시안 분포의 평균과 공분산을 가져와 타원 그리기
for i in range(best_n_components):
    draw_ellipse(best_gmm.means_[i], best_gmm.covariances_[i], ax=splot,
                 alpha=0.2, color='gray')

splot.set_title(f'3. 최적 GMM 결과 (n_components = {best_n_components})', fontsize=15)
splot.set_xlabel('X')
splot.set_ylabel('Y')
splot.legend()

plt.tight_layout()
plt.show()

```

-----

### 4\. 실행 결과 및 해석

위 코드를 실행하면 다음과 같은 3개의 그래프가 나타납니다.

1.  **원본 데이터:** 이차함수 형태를 따라 데이터가 분포하며, 노이즈로 인해 흩어져 있는 것을 볼 수 있습니다.
2.  **BIC & AIC 그래프:**
      * X축은 GMM의 컴포넌트 개수, Y축은 정보량 점수입니다.
      * 그래프를 보면, 컴포넌트 개수가 4일 때 **BIC(파란색 선) 값이 가장 낮아지는 '팔꿈치(elbow)' 지점**이 나타납니다.
      * AIC(빨간색 선)는 계속 감소하는 경향을 보이지만, BIC는 모델의 복잡도에 더 큰 페널티를 부여하기 때문에 컴포넌트 개수 4 이후로는 오히려 값이 증가합니다. 이는 4개 이상의 컴포넌트는 모델을 불필요하게 복잡하게만 만들 뿐이라는 것을 의미합니다.
      * 따라서 이 데이터 분포를 설명하는 데 **가장 적절한 가우시안 분포의 개수는 4개**라고 결론 내릴 수 있습니다.
3.  **최적 GMM 결과:**
      * 최적 모델(컴포넌트 4개)이 데이터를 4개의 클러스터로 분류한 결과입니다.
      * 각기 다른 색상의 점들은 서로 다른 가우시안 분포에 속한다고 판단된 데이터들을 의미합니다.
      * 회색 타원은 GMM이 학습한 **4개의 가우시안 분포**를 시각적으로 나타낸 것입니다. 이 타원들이 모여 전체적인 이차함수 곡선 형태를 효과적으로 근사하고 있음을 확인할 수 있습니다.

### 5\. 결론 🎓

이번 실험을 통해 우리는 다음과 같은 사실을 확인했습니다.

  * **GMM은 비선형 데이터 분포를 여러 개의 가우시안 분포의 혼합으로 효과적으로 모델링할 수 있습니다.**
  * **BIC와 AIC는 GMM의 최적 컴포넌트 개수를 찾는 데 유용한 정량적 평가 지표입니다.** 특히 BIC는 모델의 복잡도를 효과적으로 제어하여 과적합(overfitting)을 방지하는 데 도움을 줍니다.
  * **시각화는 GMM의 성능을 직관적으로 이해하고 평가하는 데 매우 강력한 도구입니다.**

이처럼 체계적인 실험 설계와 평가 지표, 그리고 시각화를 통해 가우시안 믹스쳐 모델을 효과적으로 평가하고 최적의 모델을 선택할 수 있습니다.